{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arL99aOJUyXl"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gy2VgCqVHT8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Invoke the LLM to generate response to the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZjTvnJPVR7H",
        "outputId": "ad678b23-ce71-4acf-8495-b9727cd5d9d6"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"What is meant by the term 'machine learning'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfNQP9jbhYCW"
      },
      "source": [
        "## Creating chain with LCEL (LangChain Expression Language)\n",
        "\n",
        "LCEL is now the default way to create chains in LangChain. It has a more pipeline-like syntax and allows you to modify already-existing chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc8OnZCxYvbT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an English-French translator that return whatever the user says in French\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chaining the prompt and language model to generate a response is simple. Here's how you can do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26-oW1y_Z6hJ"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ_tWUEVZ-uA",
        "outputId": "45493a04-fbdb-4179-9c63-1cfba4af05dc"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\n",
        "    \"input\": \"I love going to the beach\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_xGLWivbeOf"
      },
      "outputs": [],
      "source": [
        "# add output parser to the chain to get the output in a string format\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr5pD5_-ccrY"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OBT74zgTch-D",
        "outputId": "5522a730-ffc7-4ab4-c422-e70f46303b06"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\n",
        "    \"input\": \"The weather is nice today\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8YDHxCZANdP",
        "outputId": "e23e6620-b58b-435f-9e88-98ecaa5f2e4f"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"What is new in JavaScript?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPF1D7XzhoN-"
      },
      "source": [
        "## Creating a Retrieval Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HziCjCdGj4X3"
      },
      "source": [
        "\n",
        "## 3.1 Load the source documents\n",
        "\n",
        "First, we will have to load the documents that will enrich our LLM prompt. We will use [this blog post](https://blog.langchain.dev/langchain-v0-1-0/) from LangChain's official website explaining the new release. OpenAI's models were not trained on this content, so the only way to ask questions about it is to build a RAG chain.\n",
        "\n",
        "The first thing to do is to load the blog content to our vector store. We will use beautiful soup to scrap the blog post. Then we will store it in a FAISS vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndg7ytihAs2r",
        "outputId": "5e18451c-af5d-48a6-9125-735c57fbb998"
      },
      "outputs": [],
      "source": [
        "# retrieval chain\n",
        "\n",
        "! pip install PyPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H69g1ttYA47U",
        "outputId": "83dda3a1-ca3b-4427-ad14-ca4ad8740d8b"
      },
      "outputs": [],
      "source": [
        "# importing faiss-cpu for vector database\n",
        "! pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJBh0nZpA_jH"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./pdf/monopoly.pdf\")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-CjL7LPCZDa",
        "outputId": "f649b58b-6b85-4d64-9104-ac3c5f9554ab"
      },
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc3aDflhCZwk"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQER2E4bCwp1"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khPB0xuDDM8m",
        "outputId": "f7590ed8-7f24-451e-c255-309f012dfb9d"
      },
      "outputs": [],
      "source": [
        "splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W7kj0RoDNzV"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embeddings=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtWWJQzhiqHl"
      },
      "source": [
        "## Creating a Context-Aware LLM Chain\n",
        "\n",
        "Here we create a chain that will answer a question given a context. For now, we are passing the context manually, but in the next step we will pass in the documents fetched from the vector store we created above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrf_EX26DRHO"
      },
      "outputs": [],
      "source": [
        "# create chain for documents\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "template = \"\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2caJnGH-FFV0",
        "outputId": "8552a967-5843-47a3-a96f-1f7d6c43e7fa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_chain.invoke({\n",
        "    \"input\": \"what is langchain 0.1.0?\",\n",
        "    \"context\": [Document(page_content=\"langchain 0.1.0 is the new version of a llm app development framework.\")]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oW_TGIBjj_c"
      },
      "source": [
        "## 3.2 Create the RAG Chain\n",
        "\n",
        "RAG stands for Retrieval-Augmented Generation. This means that we will enrich the prompt that we send to the LLM. We will use with the documents that wil will retrieve from the vector store for this. LangChain comes with the function `create_retrieval_chain` that allows you to create one of these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhvXHSkPHb0K"
      },
      "outputs": [],
      "source": [
        "# create retrieval chain\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtdL4bsEFrP_"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"input\": \"what is new in langchain 0.1.0\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NvL_yXp-I-AG",
        "outputId": "23d86a2a-b38d-45c8-d816-018993707a11"
      },
      "outputs": [],
      "source": [
        "response['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKELJ-LVkE07"
      },
      "source": [
        "# Creating Conversational RAG Chain\n",
        "\n",
        "Now we will create exactly the same thing as above, but we will have the AI assistant take the history of the conversation into account. In short, we will build the same chain as above but with we will take into account the previous messages in these two steps of the chain:\n",
        "\n",
        "- When fetching the documents from the vector store. We will fetch documents related to the entire conversation and not just the latest message.\n",
        "- When answering the question. We will send to the LLM the history of the conversation along the context and query.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHkQopzYqlaC"
      },
      "source": [
        "## Creating a Conversation-Aware Retrieval Chain\n",
        "\n",
        "This chain will return the documents related to the entire conversation and not just the latest message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZmG7GY4I_XG"
      },
      "outputs": [],
      "source": [
        "# conversational retrieval chain\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])\n",
        "\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6zrgY-KSys4",
        "outputId": "5edefc84-6fb4-4b84-c8a2-102a615f5615"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [\n",
        "    HumanMessage(content=\"Is there anything new about Langchain 0.1.0?\"),\n",
        "    AIMessage(content=\"Yes!\")\n",
        "]\n",
        "\n",
        "retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me more about it!\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNjXf9UNq3Wv"
      },
      "source": [
        "## Using Retrieval Chain together with Document Chain\n",
        "\n",
        "Now we will create a document chain that contains a placeholder for the conversation history. \n",
        "\n",
        "This placeholder will be populated with the conversation history that we will pass as its value. We will the plug it together with the retriever chain we created above to have a conversational retrieval-augmented chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKOR41J8Vj6s"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "conversational_retrieval_chain = create_retrieval_chain(\n",
        "    retriever_chain, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm_KBfo6XxkJ"
      },
      "outputs": [],
      "source": [
        "response = conversational_retrieval_chain.invoke({\n",
        "    'chat_history': [],\n",
        "    \"input\": \"What is langchain 0.1.0 about?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOWw4-FLX-i0",
        "outputId": "f2a9a902-52b7-4ddc-d8a1-89516a56b1e9"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "09rT_Q_rYCgj",
        "outputId": "5c2f3ac1-1e22-4467-cda8-465ebb247c8f"
      },
      "outputs": [],
      "source": [
        "response['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulate conversation history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYRTU-XpYJC3"
      },
      "outputs": [],
      "source": [
        "chat_history = [\n",
        "    HumanMessage(content=\"Is there anything new on Langchain 0.1.0?\"),\n",
        "    AIMessage(content=\"Yes!\")\n",
        "]\n",
        "\n",
        "response = conversational_retrieval_chain.invoke({\n",
        "    'chat_history': chat_history,\n",
        "    \"input\": \"Tell me more about it!\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAeZs-f4YWrn",
        "outputId": "0f65666d-85ec-45a3-8e34-2084eb2f9d30"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "FI2NB_yjYc_4",
        "outputId": "5b8a2153-2703-4431-c744-766901cbeab0"
      },
      "outputs": [],
      "source": [
        "response['answer']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
